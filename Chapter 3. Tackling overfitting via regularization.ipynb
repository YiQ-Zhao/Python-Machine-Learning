{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tackling overfitting via regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a model suffers from overfitting, we also say that the model has a high variance, which can be caused by havting too many parameters that lead to a model that is too complex given the underlying data. Similarly, our model can also suffer from underfitting (high bias), which means that our model is not complex enough to capture the pattern in the training data well and therefore also suffers from low performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Variance**: Variance measures the consistency (or variability) of the model prediction for a particular sample instance if we would retrain the model multiple times, for example, on different subsets of the training dataset. We can say that the model is sensitive to the *randomness* in the training data.  \n",
    "* **Bias**: Bias measures how far off the predictions are from the correct values in general if we rebuild the model multiple times on different training datasets; bias is the measure of the *systematic error* that is not due to randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\lambda}{2}\\Vert{\\mathbf{w}}\\Vert^2 = \\frac{\\lambda}{2}\\sum^m_{j=1}w^2_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $\\lambda$ is the so-called regularization parameter.  \n",
    "In order to apply regularization, we just need to add the regularization term to the cost function that we definied for logistic regression to shrink the weights:  \n",
    "$$J(\\mathbf{w}) = \\sum^n_{i=1}[-y^{(i)}log(\\phi(z^{(i)}))-(1-y^{(i)})log(1-\\phi(z^{(i)}))]+\\frac{\\lambda}{2}\\Vert w\\Vert^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via the regularization parameter $\\lambda$, we can then control how well we fit the training data while keeping the weights small. By increaseing the value of $\\lambda$, we increase the regularization strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `C` that is implemented for the `LogisticRegression` class in scikit-learn comes from a convention in support vector machines. `C` is directly related to the regularization parameter $\\lambda$, which is its inverse   \n",
    "$$C = \\frac{1}{\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Did we mention that we could use L2 to handle overfitting in linear regression class?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
